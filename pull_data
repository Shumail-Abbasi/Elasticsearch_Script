import requests
import json
from datetime import datetime, timedelta
import time

# --- Config ---
ELASTIC_URL = "https://your-elasticsearch-url:9200"
API_KEY = "YOUR_BASE64_API_KEY"

HEADERS = {
    "Authorization": f"ApiKey {API_KEY}",
    "Content-Type": "application/json"
}

# --- Logging ---
def log(message):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

# --- Build Queries ---
def build_winsec_query():
    return {
        "_source": ["host.name", "source.domain"],
        "size": 10000,
        "query": {
            "bool": {
                "must": [
                    {
                        "range": {
                            "@timestamp": {
                                "gte": ninety_days_ago.isoformat(),
                                "lte": now.isoformat()
                            }
                        }
                    },
                    {"match": {"event.code": "4624"}}
                ],
                "must_not": [
                    {"terms": {"source.domain": ["", "-", None]}}
                ]
            }
        }
    }

def build_nac_query():
    return {
        "_source": ["host.name", "host.mac", "host.os.family", "source.domain"],
        "size": 10000,
        "query": {
            "bool": {
                "must": [],
                "must_not": [
                    {"terms": {"source.domain": ["", "-", None]}}
                ]
            }
        }
    }

def build_fleet_query():
    return {
        "_source": [
            "host.name", "host.ip", "last_checkin_status",
            "local_metadata.os.family", "local_metadata.os.full", "tags",
            "local_metadata.host.name"
        ],
        "size": 10000,
        "query": {
            "bool": {
                "must_not": [
                    {"terms": {"local_metadata.host.name": ["", "-", None]}}
                ]
            }
        }
    }

# --- Fetch Function ---
def fetch_data(index, query, scroll_time="2m", batch_size=1000):
    url = f"{ELASTIC_URL}/{index}/_search?scroll={scroll_time}"
    query["size"] = batch_size
    log(f"Starting scroll query on index: {index}")
    
    start_time = time.time()
    all_hits = []

    try:
        # Initial request
        resp = requests.post(url, headers=HEADERS, data=json.dumps(query), verify=False)
        resp.raise_for_status()
        response_json = resp.json()

        scroll_id = response_json.get("_scroll_id")
        hits = response_json["hits"]["hits"]
        all_hits.extend(hits)

        log(f"✓ Retrieved initial {len(hits)} hits...")

        # Continue scrolling
        while True:
            if not hits:
                break
            scroll_url = f"{ELASTIC_URL}/_search/scroll"
            scroll_query = {
                "scroll": scroll_time,
                "scroll_id": scroll_id
            }
            scroll_resp = requests.post(scroll_url, headers=HEADERS, data=json.dumps(scroll_query), verify=False)
            scroll_resp.raise_for_status()
            response_json = scroll_resp.json()
            hits = response_json["hits"]["hits"]
            scroll_id = response_json.get("_scroll_id")
            if hits:
                log(f"→ Retrieved {len(hits)} more hits...")
                all_hits.extend(hits)
            else:
                break

        end_time = time.time()
        log(f"✓ Scroll complete: {len(all_hits)} total hits from '{index}' in {end_time - start_time:.2f} seconds")
        return all_hits

    except requests.exceptions.RequestException as e:
        log(f"✗ Scroll error on index '{index}': {e}")
        return []


# --- Time Range (90 Days) ---
now = datetime.utcnow()
ninety_days_ago = now - timedelta(days=90)

# --- Build and Fetch ---
log("Building queries...")
winsec_query = build_winsec_query()
nac_query = build_nac_query()
fleet_query = build_fleet_query()
log("✓ Queries built.\n")

log("Fetching data from indices...\n")
winsec_data = fetch_data("windows_security", winsec_query)
nac_data = fetch_data("NAC_inventory", nac_query)
fleet_data = fetch_data("fleet_inventory", fleet_query)
log("✓ Data fetching complete.\n")

# --- Deduplication ---
log("Extracting and deduplicating names...")
start = time.time()
all_names = set()

for doc in winsec_data:
    src_domain = doc["_source"].get("source", {}).get("domain")
    if src_domain and src_domain.strip() and src_domain != "-":
        all_names.add(src_domain)

for doc in nac_data:
    src_domain = doc["_source"].get("source", {}).get("domain")
    if src_domain and src_domain.strip() and src_domain != "-":
        all_names.add(src_domain)

for doc in fleet_data:
    local_name = doc["_source"].get("local_metadata", {}).get("host", {}).get("name")
    if local_name and local_name.strip() and local_name != "-":
        all_names.add(local_name)

end = time.time()
unique_names = sorted(all_names)
log(f"✓ Found {len(unique_names)} unique names in {end - start:.2f} seconds\n")

# --- Output ---
log("Final unique names:")
for name in unique_names:
    print(" -", name)

log("Script completed successfully.")
